# ğŸ•·ï¸ Simple C++ Web Crawler

## ğŸ“Œ Overview
This is a **basic web crawler written in C++** that:
- Downloads HTML content from a given URL using `wget`
- Extracts links (`href=...`) from the downloaded pages
- Recursively crawls those links up to a specified depth
- Counts word frequencies on each page (ignoring common words)
- Stores visited URLs to prevent re-downloading

It uses:
- A **custom generic hash table** (`Hash`) to store visited URLs and word frequencies
- **Custom file handling** utilities to create and check directories
- **`wget`** (via `system()` calls) for downloading HTML files

---

## ğŸ“‚ Project Structure
/genericList/components/List.h â†’ Generic linked list implementation
/genericHash/components/hash.h â†’ Generic hash table implementation
/fileHandling/components/fileHandling.h â†’ File handling utilities
crawler.cpp â†’ Main crawler class
main.cpp â†’ Program entry point


---

## âš™ï¸ Requirements
- **C++ compiler** (g++ recommended, C++11 or newer)
- **wget** installed on your system
- Linux, macOS, or WSL on Windows

---

## ğŸš€ How to Build & Run

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
2ï¸âƒ£ Compile
Make sure all headers and .cpp files are in the correct folders:

g++ -std=c++11 main.cpp crawler.cpp -o crawler
3ï¸âƒ£ Run

./crawler <start_url> <target_directory> <max_depth>
Where:

start_url â†’ URL where crawling begins

target_directory â†’ Folder name (inside public/) where HTML will be stored

max_depth â†’ Maximum crawl depth (0 = only start page)

Example:

./crawler "https://example.com" "example_dir" 2
This will:

Create public/example_dir if it doesnâ€™t exist

Download https://example.com

Crawl extracted links up to depth 2

Count most frequent words on each page

-----
### Features
âœ… Avoids revisiting the same URL
âœ… Skips invalid/unreachable URLs
âœ… Limits crawl depth
âœ… Saves HTML locally
âœ… Finds most frequent word per page

